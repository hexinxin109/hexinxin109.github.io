<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>YOLO | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="预备知识 one&#x2F;two-stage 目标检测方法的区别和优缺点？目标检测是定位图像或者视频中感兴趣的物体，同时检测出他们的位置和大小。而one&#x2F;two-stage表示这个过程是一步到位还是两步到位，即two stage算法需要先生成一个可能包含待检物体的预选框，然后再进行细粒度的物体检测；反之，one stage算法则是直接在网络中提取特征来预测物体的分类和位置。深度学习时代常见的two sta">
<meta property="og:type" content="article">
<meta property="og:title" content="YOLO">
<meta property="og:url" content="http://example.com/2022/04/22/yolo/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="预备知识 one&#x2F;two-stage 目标检测方法的区别和优缺点？目标检测是定位图像或者视频中感兴趣的物体，同时检测出他们的位置和大小。而one&#x2F;two-stage表示这个过程是一步到位还是两步到位，即two stage算法需要先生成一个可能包含待检物体的预选框，然后再进行细粒度的物体检测；反之，one stage算法则是直接在网络中提取特征来预测物体的分类和位置。深度学习时代常见的two sta">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-04-22T07:35:42.619Z">
<meta property="article:modified_time" content="2022-05-08T14:50:04.528Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-yolo" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/04/22/yolo/" class="article-date">
  <time class="dt-published" datetime="2022-04-22T07:35:42.619Z" itemprop="datePublished">2022-04-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      YOLO
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h4 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h4><ol>
<li>one/two-stage 目标检测方法的区别和优缺点？<br>目标检测是定位图像或者视频中感兴趣的物体，同时检测出他们的位置和大小。而one/two-stage表示这个过程是一步到位还是两步到位，即two stage算法需要先生成一个可能包含待检物体的预选框，然后再进行细粒度的物体检测；反之，one stage算法则是直接在网络中提取特征来预测物体的分类和位置。<br>深度学习时代常见的two stage算法有R-CNN、Fast-RCNN、Faster-RCNN、Mask-RCNN系列，而对应的one stage算法有YOLO、SSD等。<br><strong>优缺点对比：</strong></li>
<li>one-stage 速度快，适合做实时检测任务。效果通常情况下不会太好。Tiny YOLO的FPS可以达到200，简化网络速度会更快，但是与效果成反比。</li>
<li>one-stage是直接通过主干网络给出类别和位置信息，没有使用RPN网络，什么是RPN？(区域建议网络)<br>RPN(Region Proposal Network)，RPN的输入是feature maps，其本质可以认为是“基于滑窗的无类别目标检测器”。RPN包含三个部分，头部是通过1*1卷积核获得anchor-一堆有编号有坐标的box；中部是利用分类分支和边界回归分支对这些anchor进行修正；末端对分支结果进行汇总来对anchor进行初步筛除（越界+NMS）和初步偏移（回归结果）得到proposal。最后proposal被输入到PoIPooling进行归一化并进行后续的分类回归操作。</li>
<li>two-stage具体怎么实现的？<br>对于输入的图片，首先通过卷积神经网络进行深度特征的提取-主干网络；接着通过RPN网络来进行产生候选区域，并完成区域的分类-背景和目标，同时也会对目标的位置进行初步的预测。接下来使用Roi_pooling层得到候选目标，并对应到特征图上的特征区域进行回归和分类，最终得到目标类别的判定和目标位置的确定。<h4 id="YOLO系列-v1-v5-you-only-look-once"><a href="#YOLO系列-v1-v5-you-only-look-once" class="headerlink" title="YOLO系列 v1-v5 (you only look once)"></a>YOLO系列 v1-v5 (you only look once)</h4></li>
</ol>
<p><strong>任务重申：</strong>需要在图片中找到物体并给出对应的类别与位置。<br>核心就是只需要浏览一次就可以识别出图像中物体的类别与位置，因此输入region-free的方法，相对于region-based方法而言不需要预估计出可能存在目标的region。<br><strong>核心思想：</strong> one-stage+回归</p>
<h5 id="YOLOV1"><a href="#YOLOV1" class="headerlink" title="YOLOV1"></a>YOLOV1</h5><ol>
<li>算法的出发点：<br>用单个CNN网络直接对图像做回归，即将检测问题转化成回归问题(x,y,w,h)，主要针对的是视频的实时检测任务。</li>
<li>算法的逻辑简述如下：<br>(1) 将图像448x448x3分为sxs个小格子，每个格子负责预测中心点落在该格子中的物体，即给出一个置信度值来表示是否包含物体；<br>(2) 对于置信度高于阈值的格子，为当前格子提供两个经验候选框并与gt计算交并比，对交并比更大的格子进行回归(微调格子的位置的尺寸x,y,w,h)；<br>(3) 最终得出的数据为sxs(bx5+c)，其中b表示每个格子提供的经验候选框数量，默认是2；c表示数据集中的类别数量，默认是20；sxs表示的是小格子的数量，默认是7*7；</li>
<li>损失函数<br>针对sxs(bx5+c)个输出值，每一个格子会有b类预测框，但是算法只选出IoU最大的预测框进行回归，5表示的是(位置，尺寸，置信度:x,y,w,h,score)。<br>(1) 位置误差 分为坐标xy的损失与尺寸wh的损失。x,y直接计算距离；w,h需先开根号后计算距离；因为考虑到小的偏差对于小物体来说影响比大物体更大，比如2x2的物体偏差一个像素与20x20的物体偏差一个像素的误差程度是不一样的。让损失函数在尺寸较小时比较敏感，即相同差异在w,h较小时造成的损失较小。<br>(2) 置信度损失 分为包含物体的损失函数与不包含物体的损失函数。考虑到数据集中背景信息远大于前景信息，若不分开考虑的话，其中背景的置信度趋向于0，前景的置信度趋向于1，若不分开考虑的话，损失部分会向背景信息偏移；<br>(3) 分类损失 针对预测出的类别计算损失。</li>
<li>存在问题<br>(1) 重叠在一起的物体无法被检测，因为每个格子被设置为只预测一个物体；<br>(2) 小物体检测效果一般，因为经验候选框的大小是固定的且单一，对于小物体不太友好；<h5 id="YOLOV2"><a href="#YOLOV2" class="headerlink" title="YOLOV2"></a>YOLOV2</h5></li>
<li>相对于V1改进的细节<br>(1) 舍弃Dropout，卷积后全部加入Batch Normalization，收敛相对更容易，可以提升2%的mAP；<br>(2) 更大的分辨率，由于v1全程是用224x224的数据进行训练，测试是用448x448的数据进行测试，考虑到尺寸之间的差异可能带来的问题，v2在训练完后额外进行了10次448x448的微调，可以提升4%的mAP；<br>(3) 网络结构：DarkNet19，实际输入为416x416，去除FC层，利用5次降采样得到13x13的特征图。参考VGG网络，卷积层都是小尺度的卷积核3x3，并在相邻3x3中插入1x1的卷积层来节省参数。<br>(4) 利用k-means聚类从数据集中分析出5类先验框(anchor)，相对于faster-rcnn更贴近实际。聚类中设计的距离定义为1-IoU(box,centroids)，主要是考虑到不同尺寸下相同距离的影响程度不同。<br>(5) 舍弃直接使用相对于先验框的偏移量，修改成相对于grid cell的偏移量，可以确保预测框不会偏移太大(理论上是相对于当前网格进行预测，因此实际框不能脱离网格)，利用sigmoid函数来限制预测偏移量不超过1。<br>(6) 特征融合：考虑到最后一层的感受野太大，小目标可能丢失，因此将前面的特征进行拆分并与后层拼接。(26x26x512-&gt;4x13x13x512,并与最后的13x13x512进行拼接)<br>(7) multi-scale：由于卷积操作对输入尺度没有要求，因此尝试在一定迭代次数后随机改变输入尺寸，包括(320~608)之间任意可以被32整除的尺寸。<h5 id="YOLOV3"><a href="#YOLOV3" class="headerlink" title="YOLOV3"></a>YOLOV3</h5></li>
<li>相对于前两个版本提出的细节<br>(1) 网络结构：从提升小目标检测的需求出发进行改进，实际输入是416x416，去除池化与全连接层，并加入若干残差网络变成DarkNet53。<br>(2) 多scale： 为了能检测到不同大小的物体，设计了3个scale(分别在对应层直接进行映射)分别针对大中小物体，同时每种scale具有三个规格box，因此一共9种box。对应层直接映射可以简单理解为v2的拼接将导致不同感受野专注的信息受损，因此v3计划在当前层直接预测。(猜测)后续层次的感受野将会得到更为全局的信息/认知，因此下一层可以通过上采样与累加的方式作用到当前层来协助预测。例如13x13直接预测；26x26层需要借助13x13的上采样结果进行拼接；52x52层需要借助26x26的上采样结果进行拼接。<br>(3) 输出结果最后一层为13x13x3x(80+4+1)，其中3表示每个格子根据三个不同规格box进行回归，80表示类别，4表示xywh，1表示置信度。<br>(4) 通过聚类得出9种不同的先验框，分别分为三类送到13x13,26x26,52x52。<br>(5) softmax层替代：考虑到物体检测任务中可能一个物体有多个标签(猫，大猫，黄猫)，使用logistic函数来对每个类别进行判断。<h5 id="Faster-rcnn"><a href="#Faster-rcnn" class="headerlink" title="Faster-rcnn"></a>Faster-rcnn</h5>设置不同比例(w:h)的9种先验框，分别三种不同尺度下的1：1，1：2，2：1先验框。<h5 id="检测任务评价指标"><a href="#检测任务评价指标" class="headerlink" title="检测任务评价指标"></a>检测任务评价指标</h5></li>
</ol>
<p><strong>预置知识：</strong><br>(1) IoU-交并比 交叉区域框之间的交集与并集之比；<br>(2) TP FP TN FN T/F表示true/false 表示判断是否正确； P/N表示positive/negative 表示判断成正例还是负例；<br>精度 = TP/(TP+FP);召回率 = TP/(TP+FN); 比如TP表示将当前样本判断为正样本的过程是正确的，即正样本-&gt;正样本的过程。<br>(3) NMS-非极大值抑制 当预测框之间交叉区域较大的时候，保留置信度更大的预测框。</p>
<p><strong>检测类常用的指标：</strong>mAP&amp;FPS<br>mAP 一般表示为PR图的点所围成的区域，可以认为是综合考虑了精度和召回率的指标.</p>
<!--one-hot编码-->

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/04/22/yolo/" data-id="cl2xeuncm0001hsu49jendqgz" data-title="YOLO" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/04/29/git/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Git
        
      </div>
    </a>
  
  
    <a href="/2022/04/20/network/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Networks</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/04/30/activation/">Activation Funtion</a>
          </li>
        
          <li>
            <a href="/2022/04/29/backbone/">backbone</a>
          </li>
        
          <li>
            <a href="/2022/04/29/git/">Git</a>
          </li>
        
          <li>
            <a href="/2022/04/22/yolo/">YOLO</a>
          </li>
        
          <li>
            <a href="/2022/04/20/network/">Networks</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>