<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="InceptionTransformer抛弃了传统的CNN和RNN，网络结构完全由attention组成。具体的说，是由self-attention和前馈神经网络组成。 NeRF在于非显式地(MLP)将一个复杂的静态场景用一个神经网络来建模，训练之后，从任意角度渲染出真实的场景图片。需要提供一个静态场景和大量相机参数已知的图片。 Meta learningBertfastText &amp; wo">
<meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/backbone.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="InceptionTransformer抛弃了传统的CNN和RNN，网络结构完全由attention组成。具体的说，是由self-attention和前馈神经网络组成。 NeRF在于非显式地(MLP)将一个复杂的静态场景用一个神经网络来建模，训练之后，从任意角度渲染出真实的场景图片。需要提供一个静态场景和大量相机参数已知的图片。 Meta learningBertfastText &amp; wo">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-05-01T15:08:34.179Z">
<meta property="article:modified_time" content="2022-05-01T15:08:34.179Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="page-" class="h-entry article article-type-page" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/backbone.html" class="article-date">
  <time class="dt-published" datetime="2022-05-01T15:08:34.179Z" itemprop="datePublished">2022-05-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h5 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h5><h5 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h5><p>抛弃了传统的CNN和RNN，网络结构完全由attention组成。具体的说，是由self-attention和前馈神经网络组成。</p>
<h5 id="NeRF"><a href="#NeRF" class="headerlink" title="NeRF"></a>NeRF</h5><p>在于非显式地(MLP)将一个复杂的静态场景用一个神经网络来建模，训练之后，从任意角度渲染出真实的场景图片。需要提供一个静态场景和大量相机参数已知的图片。</p>
<h5 id="Meta-learning"><a href="#Meta-learning" class="headerlink" title="Meta learning"></a>Meta learning</h5><h5 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h5><h5 id="fastText-amp-word2vec"><a href="#fastText-amp-word2vec" class="headerlink" title="fastText &amp; word2vec"></a>fastText &amp; word2vec</h5><h5 id="CNN-amp-RNN"><a href="#CNN-amp-RNN" class="headerlink" title="CNN &amp; RNN"></a>CNN &amp; RNN</h5><h5 id="LSTM-amp-GRU"><a href="#LSTM-amp-GRU" class="headerlink" title="LSTM &amp; GRU"></a>LSTM &amp; GRU</h5><h5 id="attention-amp-self-attention"><a href="#attention-amp-self-attention" class="headerlink" title="attention &amp; self-attention"></a>attention &amp; self-attention</h5><ol>
<li>attention(注意力机制)。<br>核心逻辑就是从关注全部到关注重点，类似于人类的注意力：将有限的注意力集中在重点信息上，从而节省资源、快速获得最有效的信息。模型复杂度小（参数少）；可以并行处理（速度快）；挑重点记忆长距离信息（效果好）。<br>原理分解：利用query(?)和key(?)进行相似度计算，得到权值；权值归一化得到可用的权重；权重与value进行加权求和。</li>
<li>N种类型</li>
</ol>
<p><strong>soft &amp; hard &amp; local:</strong> 主要区别体现在计算区域上，soft是对所有的key求权重概率再进行加权，是一种全局的计算方式；hard是针对某个key进行计算，这种方式不可导且对对齐要求极高；local是在前两者的基础上进行折中，即先通过hard定位到某个位置，再以此为中心取一个窗口区域进行attention计算。<br><strong>单层 &amp; 多层 &amp; 多头：</strong> 主要区别在结构上是否划分层次关系，单层是指用一个query对内容进行一次attention；多层是指具有层次关系的模型，以文本为例：先对每个句子使用attention计算出一个句向量，再对所有句向量通过attention计算出一个文档向量，最后利用文档向量去进一步计算任务；多头是指用多个query对内容进行多次attention，每个query关注的是不同的部分，最后进行结果拼接。<br><strong>静态 attention：</strong><br><strong>动态 attention：</strong><br><strong>self attention：</strong></p>
<h5 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h5><h5 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h5>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/backbone.html" data-id="cl2knbh5f0001k4u41vut08qx" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/04/30/activation/">Activation Funtion</a>
          </li>
        
          <li>
            <a href="/2022/04/29/git/">Git</a>
          </li>
        
          <li>
            <a href="/2022/04/20/network/">Networks</a>
          </li>
        
          <li>
            <a href="/2022/04/18/transform/">Wavelet &amp; Fourier Transfrom</a>
          </li>
        
          <li>
            <a href="/2022/04/14/optimizers/">Optimizers</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>